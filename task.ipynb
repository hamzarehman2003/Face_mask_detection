{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "Current CUDA device: NVIDIA GeForce RTX 4060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "print(\"Current CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and numerical libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch libraries for model building, training, and evaluation\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Tokenization and text preprocessing\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# BLEU score for evaluation\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data from text files\n",
    "def read_corpus(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return lines\n",
    "\n",
    "english_sentences = read_corpus(\"Dataset/english-corpus.txt\")\n",
    "urdu_sentences = read_corpus(\"Dataset/urdu-corpus.txt\")\n",
    "\n",
    "# Check if both files have the same number of sentences\n",
    "assert len(english_sentences) == len(urdu_sentences), \"Mismatch in number of sentences!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: [['is', 'zain', 'your', 'nephew'], ['i', 'wish', 'youd', 'trust', 'me'], ['did', 'he', 'touch', 'you']]\n",
      "Urdu: [['زین', 'تمہارا', 'بھتیجا', 'ہے'], ['کاش', 'تم', 'مجھ', 'پر', 'بھروسہ', 'کرتے'], ['کیا', 'اس', 'نے', 'آپ', 'کو', 'چھوا']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    # Lowercase and remove punctuation (you may need to handle language-specific punctuation for Urdu)\n",
    "    text = text.lower().strip()\n",
    "    # Using regex to keep only words (this can be adjusted for Urdu)\n",
    "    tokens = re.findall(r'\\w+', text)\n",
    "    return tokens\n",
    "\n",
    "# Tokenize each sentence from both corpora\n",
    "english_tokens = [tokenize(sentence) for sentence in english_sentences]\n",
    "urdu_tokens = [tokenize(sentence) for sentence in urdu_sentences]\n",
    "\n",
    "# Let’s inspect some examples\n",
    "print(\"English:\", english_tokens[:3])\n",
    "print(\"Urdu:\", urdu_tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 5628\n",
      "Urdu vocab size: 5621\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(tokenized_sentences, min_freq=1):\n",
    "    counter = Counter()\n",
    "    for tokens in tokenized_sentences:\n",
    "        counter.update(tokens)\n",
    "        \n",
    "    # Starting index for normal words, plus special tokens\n",
    "    vocab = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
    "    index = len(vocab)\n",
    "    for word, freq in counter.items():\n",
    "        if freq >= min_freq:\n",
    "            vocab[word] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "eng_vocab = build_vocab(english_tokens)\n",
    "urd_vocab = build_vocab(urdu_tokens)\n",
    "\n",
    "print(\"English vocab size:\", len(eng_vocab))\n",
    "print(\"Urdu vocab size:\", len(urd_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sequence Example: [1, 4, 5, 6, 7, 2]\n",
      "Urdu Sequence Example: [1, 4, 5, 6, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "def sentence_to_indices(tokens, vocab):\n",
    "    # Start with <sos> and end with <eos>\n",
    "    indices = [vocab.get(\"<sos>\")]\n",
    "    indices += [vocab.get(token, vocab.get(\"<unk>\")) for token in tokens]\n",
    "    indices.append(vocab.get(\"<eos>\"))\n",
    "    return indices\n",
    "\n",
    "eng_sequences = [sentence_to_indices(tokens, eng_vocab) for tokens in english_tokens]\n",
    "urd_sequences = [sentence_to_indices(tokens, urd_vocab) for tokens in urdu_tokens]\n",
    "\n",
    "# Example: print first sequence\n",
    "print(\"English Sequence Example:\", eng_sequences[0])\n",
    "print(\"Urdu Sequence Example:\", urd_sequences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded English batch shape: torch.Size([24525, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    tensor_list = [torch.tensor(seq, dtype=torch.long) for seq in sequences]\n",
    "    padded_tensor = pad_sequence(tensor_list, batch_first=True, padding_value=padding_value)\n",
    "    return padded_tensor\n",
    "\n",
    "# Example for a batch (or full dataset if memory permits)\n",
    "eng_padded = pad_sequences(eng_sequences, padding_value=eng_vocab[\"<pad>\"])\n",
    "urd_padded = pad_sequences(urd_sequences, padding_value=urd_vocab[\"<pad>\"])\n",
    "\n",
    "print(\"Padded English batch shape:\", eng_padded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(5628, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(5621, 256)\n",
      "    (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "    (fc_out): Linear(in_features=512, out_features=5621, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Encoder model\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, src_len]\n",
    "        embedded = self.embedding(src)  # [batch_size, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "# Decoder model\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input shape: [batch_size] -> we add a time-dim (length 1)\n",
    "        input = input.unsqueeze(1)\n",
    "        embedded = self.embedding(input)  # [batch_size, 1, emb_dim]\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Seq2Seq model that ties encoder and decoder together\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.embedding.num_embeddings\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encode the source sentence\n",
    "        hidden, cell = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t] = output\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "# Define hyperparameters and instantiate models:\n",
    "INPUT_DIM = len(eng_vocab)\n",
    "OUTPUT_DIM = len(urd_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Print model summary\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sequences, trg_sequences):\n",
    "        assert len(src_sequences) == len(trg_sequences), \"Datasets must be of equal length\"\n",
    "        self.src_sequences = src_sequences\n",
    "        self.trg_sequences = trg_sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert sequences (lists of indices) to tensors\n",
    "        src_tensor = torch.tensor(self.src_sequences[idx], dtype=torch.long)\n",
    "        trg_tensor = torch.tensor(self.trg_sequences[idx], dtype=torch.long)\n",
    "        return src_tensor, trg_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unpack a batch of source and target sequences\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    \n",
    "    # Pad sequences using the corresponding pad token indices from vocabularies.\n",
    "    # Make sure that eng_vocab[\"<pad>\"] and urd_vocab[\"<pad>\"] exist.\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=eng_vocab[\"<pad>\"])\n",
    "    trg_batch = pad_sequence(trg_batch, batch_first=True, padding_value=urd_vocab[\"<pad>\"])\n",
    "    \n",
    "    return src_batch, trg_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming eng_sequences and urd_sequences were generated earlier from your corpus\n",
    "dataset = TranslationDataset(eng_sequences, urd_sequences)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss = 4.417996755013099\n",
      "Epoch 2: Loss = 3.365813548437464\n",
      "Epoch 3: Loss = 2.708676909530023\n",
      "Epoch 4: Loss = 2.1652185531452086\n",
      "Epoch 5: Loss = 1.736251979328041\n",
      "Epoch 6: Loss = 1.3805348039295091\n",
      "Epoch 7: Loss = 1.079663947477179\n",
      "Epoch 8: Loss = 0.8477709890966154\n",
      "Epoch 9: Loss = 0.6804032131466126\n",
      "Epoch 10: Loss = 0.5483123998909295\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=urd_vocab[\"<pad>\"])\n",
    "\n",
    "# Dummy training loop for illustration\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src_batch, trg_batch in dataloader:  # assuming you have a DataLoader set up\n",
    "        src_batch = src_batch.to(device)\n",
    "        trg_batch = trg_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src_batch, trg_batch, teacher_forcing_ratio=0.5)\n",
    "        # Output shape: [batch_size, trg_len, output_dim]\n",
    "        \n",
    "        # Flatten the output and target for computing loss\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg_batch[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}: Loss = {epoch_loss / len(dataloader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Sentence   : she cooks for him\n",
      "Reference Urdu     : وہ اس کے لیے کھانا پکاتی ہے۔\n",
      "Predicted Urdu     : <sos> وہ اس کے لیے کھانا پکاتی ہے <eos>\n",
      "BLEU Score         : 0.7259795291154771\n",
      "--------------------------------------------------\n",
      "English Sentence   : they are lost\n",
      "Reference Urdu     : وہ کھو گئے ہیں\n",
      "Predicted Urdu     : <sos> وہ کھو رہے ہیں <eos>\n",
      "BLEU Score         : 8.38826642100846e-155\n",
      "--------------------------------------------------\n",
      "English Sentence   : where is she\n",
      "Reference Urdu     : وہ کہاں ہے\n",
      "Predicted Urdu     : <sos> وہ کہاں ہے <eos>\n",
      "BLEU Score         : 6.86809206056511e-78\n",
      "--------------------------------------------------\n",
      "English Sentence   : i understand you\n",
      "Reference Urdu     : میں سمجھتا/سمجھتی ہوں\n",
      "Predicted Urdu     : <sos> میں تمہیں سمجھتی ہوں <eos>\n",
      "BLEU Score         : 8.38826642100846e-155\n",
      "--------------------------------------------------\n",
      "English Sentence   : the house is vacant\n",
      "Reference Urdu     : یہ گھر خالی ہے۔\n",
      "Predicted Urdu     : <sos> یہ گھر خالی ہے <eos>\n",
      "BLEU Score         : 0.5081327481546147\n",
      "--------------------------------------------------\n",
      "English Sentence   : i wrote it for tom\n",
      "Reference Urdu     : میں نے اسے ٹام کے لیے لکھا\n",
      "Predicted Urdu     : <sos> میں نے اسے ٹام کے لیے لکھا <eos>\n",
      "BLEU Score         : 0.7259795291154771\n",
      "--------------------------------------------------\n",
      "English Sentence   : she is always scared\n",
      "Reference Urdu     : وہ ہمیشہ ڈرتا ہے\n",
      "Predicted Urdu     : <sos> وہ ہمیشہ ڈرتا ہے <eos>\n",
      "BLEU Score         : 0.5081327481546147\n",
      "--------------------------------------------------\n",
      "English Sentence   : the scorching sun\n",
      "Reference Urdu     : چلچلاتی دھوپ\n",
      "Predicted Urdu     : <sos> چلچلاتی دھوپ <eos>\n",
      "BLEU Score         : 9.53091075863908e-155\n",
      "--------------------------------------------------\n",
      "English Sentence   : he found my bicycle\n",
      "Reference Urdu     : اسے میری سائیکل مل گئی۔\n",
      "Predicted Urdu     : <sos> اسے میری سائیکل مل گئی <eos>\n",
      "BLEU Score         : 0.6147881529512643\n",
      "--------------------------------------------------\n",
      "English Sentence   : slow down\n",
      "Reference Urdu     : سست\n",
      "Predicted Urdu     : <sos> نیچے <eos>\n",
      "BLEU Score         : 0\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hamza\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Hamza\\anaconda3\\envs\\deep_learning\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Function to evaluate a single sentence using the model.\n",
    "def evaluate_sentence(model, sentence, eng_vocab, urd_vocab, max_len=50):\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)  # tokenize the input sentence (should include lowercasing etc.)\n",
    "    indices = sentence_to_indices(tokens, eng_vocab)  # convert tokens to indices, adding <sos> and <eos>\n",
    "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "    # Start decoding with <sos>\n",
    "    trg_indices = [urd_vocab[\"<sos>\"]]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]], dtype=torch.long).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        next_token = output.argmax(1).item()\n",
    "        trg_indices.append(next_token)\n",
    "        if next_token == urd_vocab[\"<eos>\"]:\n",
    "            break\n",
    "            \n",
    "    return trg_indices\n",
    "\n",
    "# Create an inverse vocabulary for Urdu for converting indices back to words.\n",
    "inv_urd_vocab = {v: k for k, v in urd_vocab.items()}\n",
    "\n",
    "# Choose 10 random indices from the english dataset.\n",
    "num_samples = 10\n",
    "random_indices = random.sample(range(len(english_sentences)), num_samples)\n",
    "\n",
    "# Loop over the selected indices, process each sentence, and print the results.\n",
    "for i in random_indices:\n",
    "    eng_sentence = english_sentences[i]\n",
    "    ref_urdu_sentence = urdu_sentences[i]\n",
    "    \n",
    "    # Predict the translation for the given English sentence.\n",
    "    predicted_indices = evaluate_sentence(model, eng_sentence, eng_vocab, urd_vocab)\n",
    "    predicted_translation = \" \".join([inv_urd_vocab.get(idx, \"<unk>\") for idx in predicted_indices])\n",
    "    \n",
    "    # For BLEU score, tokenize both the predicted and reference sentences.\n",
    "    candidate_tokens = [inv_urd_vocab.get(idx, \"<unk>\") for idx in predicted_indices]\n",
    "    reference_tokens = tokenize(ref_urdu_sentence)\n",
    "    bleu = sentence_bleu([reference_tokens], candidate_tokens)\n",
    "    \n",
    "    print(\"English Sentence   :\", eng_sentence)\n",
    "    print(\"Reference Urdu     :\", ref_urdu_sentence)\n",
    "    print(\"Predicted Urdu     :\", predicted_translation)\n",
    "    print(\"BLEU Score         :\", bleu)\n",
    "    print(\"-\" * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
